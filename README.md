# nlp-study

## Attention Is All You Need (나동빈)

💻 code from [github](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb) - 나동빈

📄 [Original Paper Link](https://arxiv.org/abs/1706.03762)

📹 [Paper Review Video](https://www.youtube.com/watch?v=AA621UofTUA&ab_channel=%EB%8F%99%EB%B9%88%EB%82%98) - 나동빈

📝 [Summary PDF](https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/lecture_notes/Transformer.pdf) - 나동빈

<br>

## BERT (E-book)

💻 code from [wikidocs](https://wikidocs.net/109251) - 딥러닝을 이용한 자연어 처리 입문

💻 code from [github](https://github.com/LaJeremi/Tensorflow-nlp-tutorial-Practice-/blob/0c862ebe1966546b0b5b95aed26a36af0bb560d6/18.%20Fine-tuning%20BERT%20(Cls%2C%20NER%2C%20NLI)/%2018_03_google_bert_nsmc_tpu.ipynb) - LaJeremi

<br>

## BERT (SKplanet Tacademy)

💻 code from [google drive](https://drive.google.com/drive/folders/1QQphR2tmk5g6BheZKZ5q8WhX5yixV8xZ) - SKplanet Tacademy

📹 Explanation Video [<1. BERT>](https://www.youtube.com/watch?v=riGc8z3YIgQ&t=2s&ab_channel=SKplanetTacademy) 
[<2. KorBERT>](https://www.youtube.com/watch?v=PzvKDpQgNzc&ab_channel=SKplanetTacademy) 
[<3. BERT Training>](https://www.youtube.com/watch?v=S42vDzJExIA&t=368s&ab_channel=SKplanetTacademy) - SKplanet Tacademy

> Tensorflow ver1으로 작성되어, ver2로 업그레이드 하는데 실패함 (contrib)

```python
import tensorflow.compat.v1 as tf

tf.contrib # error
```

> Unable to render rich display (github colab display가 안됨)
> 
<br>

## BERT (graycode)

💻 code from [github](https://github.com/graykode/nlp-tutorial) - graycode

<br>

## BERT (김웅곤)

💻 code from [github](https://github.com/kimwoonggon/publicservant_AI) - 김웅곤

📹 Explanation Video [<1. naver_sentiment>](https://www.youtube.com/watch?v=OOfCI8R0jr8&ab_channel=%EA%B9%80%EC%9B%85%EA%B3%A4) 
[<2. Q&A>](https://www.youtube.com/watch?v=LuApA264Wbs&ab_channel=%EA%B9%80%EC%9B%85%EA%B3%A4) - 김웅곤

<br>

## BERT (google research)

💻 code from [github](https://github.com/google-research/bert) (BERT-Base, Multilingual Cased) - google research

📄 [Original Paper Link](https://arxiv.org/abs/1810.04805)

<br>

## BPE: Byte Pair Encoding

💻 code from [github](https://github.com/BurningFalls/nlp-study/blob/main/Byte%20Pair%20Encoding/BPE.ipynb) - 이성주

📄 [Original Paper Link](https://arxiv.org/abs/1508.07909)

<br>

## Transformer

💻 code from [github](https://github.com/graykode/nlp-tutorial) - graycode

📝 library lecture from [wikidocs](https://wikidocs.net/book/8056)
